# External libraries
import sys
import types
from tqdm import tqdm
from copy import deepcopy

# Rotifer
import rotifer
import rotifer.db.core
logger = rotifer.logging.getLogger(__name__)

# GeneNeighborhoodCursor dependencies
import numpy as np
import pandas as pd
from rotifer.db.ncbi import utils as rdnu
from rotifer.genome.utils import seqrecords_to_dataframe

class SimpleParallelProcessCursor(rotifer.db.core.BaseCursor):
    """
    Split input into batches and fetch data using parallel processes.

    Parameters
    ----------
    batch_size: int, default 1
      Number of accessions per batch
    threads: integer, default 3
      Number of simultaneous threads to run
    progress: boolean, deafult False
      Whether to print a progress bar
    tries: int, default 3
      Number of attempts to download each batch

    """
    def __init__(self, progress=True, tries=3, batch_size=None, threads=10, *args, **kwargs):
        super().__init__(progress=progress, *args, **kwargs)
        self.tries = tries
        self.batch_size = batch_size
        self.threads = threads

    def __getitem__(self, accession, *args, **kwargs):
        """
        Dictionary-like access to a database entry.

        Parameters
        ----------
        accession: string
          Database entry identifier.

        Returns
        -------
        Objects created by the parser() method
        """
        obj = None
        error = None
        targets = self.parse_ids(accession)
        for attempt in range(0,self.tries):
            try:
                stream = self.fetcher(targets, *args, **kwargs)
            except:
                error = f'{sys.exc_info()[1]}'
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(error)
                giveup = False
                for substr in self.giveup:
                    if substr in error:
                        giveup = True
                        break
                if giveup:
                    break
                else:
                    continue

            if isinstance(stream,types.NoneType):
                error = f"Empty stream for accession(s) {accession}"
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(error)

            try:
                obj = self.parser(stream, targets, *args, **kwargs)
                break
            except:
                error = f'{sys.exc_info()[1]}'
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(f"Parser failed for {accession}")
                giveup = False
                for substr in self.giveup:
                    if substr in error:
                        giveup = True
                        break
                if giveup:
                    break
                else:
                    continue

        # Search for missing accessions
        found = set()
        if isinstance(obj,types.NoneType) or len(obj) == 0:
            self.update_missing(targets, error or "Empty object returned")
        else:
            found = self.getids(obj)
        if found:
            self.remove_missing(found)
        missing = targets - found
        if missing:
            self.update_missing(missing, error or f'Not found.')

        return obj

    def parser(self, stream, accession, *args, **kwargs):
        """
        Create instances of objects by processing the input
        stream generated by fetcher().

        This method is called by __getitem__().

        Parameters
        ----------
        stream: open file-like handle
          Object returned by fetcher().
          Should implement a file-like interface
        accession: string
          Database entry(ies) identifier(s)

        """
        raise NotImplementedError(f'Method parser() must be implemented by descendants')

    def fetcher(self, accession, *args, **kwargs):
        """
        Access remote data as a data stream.

        This method is called by __getitem__().
        """
        raise NotImplementedError(f'Method fetcher() must be implemented by descendants')

    def worker(self, accessions, *args, **kwargs):
        """
        Single-threaded method that will process the data
        batches generated by splitter().

        Parameters
        ----------
        accessions: list of strings
          List of database identifiers

        Returns
        -------
        List of objects generated by self.__getitem__()
        """
        targets = sorted(list(self.parse_ids(accessions)))
        if self.maxgetitem > 1:
            targets = [ targets[x:x+self.maxgetitem] for x in range(0,len(targets),self.maxgetitem) ]
        stack = []
        for chunk in targets:
            obj = self.__getitem__(chunk, *args, **kwargs)
            if isinstance(obj,types.NoneType):
                continue
            if len(obj) == 0:
                continue
            if isinstance(obj,list):
                stack.extend(obj)
            else:
                stack.append(obj)
        return {"result":stack,"missing":self._missing}

    def splitter(self, accessions, *args, **kwargs):
        """
        Convert a list of database identifiers to batches of objects.

        The implementation provided here divides the input into
        lists of length equal to ```batch_size``` and returns
        a generator.

        This method is called by fetchone() to define the datasets
        processed by worker() in each parallel batch.
        """
        from rotifer.devel.alpha.gian_func import chunks
        size = self.batch_size
        if size == None or size == 0:
            size = max(int(len(accessions) / self.threads),1)
        return chunks(list(accessions), size)

    def fetchone(self, accessions, *args, **kwargs):
        """
        Asynchronously fetch data in random order.

        Parameters
        ----------
        accessions: list of strings
          Database accessions

        Returns
        -------
        A generator
        """
        from concurrent.futures import ProcessPoolExecutor, as_completed

        # Split jobs and execute
        targets = self.parse_ids(accessions)
        with ProcessPoolExecutor(max_workers=self.threads) as executor:
            if self.progress:
                tqdmobj = tqdm(total=len(targets), initial=0)
            tasks = []
            missing = self.remove_missing()
            for chunk in self.splitter(list(targets), *args, **kwargs):
                tasks.append(executor.submit(self.worker, chunk))
            self.update_missing(data=missing)
            completed = set()
            for x in as_completed(tasks):
                data = x.result()
                for acc in completed.intersection(data['missing'].keys()):
                    data['missing'].pop(acc, None)
#                for s in data['missing'].iterrows():
#                    if s[0] in targets and s[0] not in completed:
#                        self.missing.loc[s[0]] = s[1].to_list()
                self.update_missing(data=data['missing'])
                result = data['result']
                if isinstance(result,types.NoneType):
                    continue
                if not isinstance(result,list):
                    result = [result]
                for obj in result:
                    found = self.getids(obj, *args, **kwargs)
                    done = targets.intersection(found) - completed
                    if len(done) > 0:
                        completed.update(done)
                        self.remove_missing(done)
                        if self.progress:
                            tqdmobj.update(len(done))
                    yield obj

    def fetchall(self, accessions, *args, **kwargs):
        """
        Fetch all data.
        Note: input order is not preserved.

        Parameters
        ----------
        accessions: list of strings
         Database entry identifiers 
        """
        return list(self.fetchone(accessions, *args, **kwargs))

class GeneNeighborhoodCursor(rotifer.db.core.BaseCursor):
    def __init__(
            self,
            column = 'pid',
            before = 7,
            after = 7,
            min_block_distance = 0,
            strand = None,
            fttype = 'same',
            eukaryotes = False,
            exclude_type = ['gene','mRNA','source'],
            autopid = True,
            codontable = 'Bacterial',
            progress=False,
            tries=3,
            batch_size=None,
            threads=10,
            *args, **kwargs):
        self._target_column = 'assembly'

        super().__init__(progress=progress, *args, **kwargs)

        self.column = column
        self.before = before
        self.after = after
        self.min_block_distance = min_block_distance
        self.strand = strand
        self.fttype = fttype
        self.eukaryotes = eukaryotes
        self.exclude_type = exclude_type
        self.autopid = autopid
        self.codontable = codontable
        self.tries = tries
        self.batch_size = batch_size
        self.threads = threads

        # Patterns expected in error messages that
        # signal for giving up missing entries
        self.giveup.update(["HTTP Error 400"])
        self.giveup.update(["no IPG","No IPG"])
        if not eukaryotes:
            self.giveup.update(["Eukaryot","eukaryot"])

    def __getitem__(self, accessions, ipgs=None):
        """
        Find gene neighborhoods in a genome.

        Returns
        -------
        rotifer.genome.data.NeighborhoodDF
        """
        objlist = seqrecords_to_dataframe([])

        # Make sure no identifiers are used twice
        targets = self.parse_ids(accessions)

        if isinstance(ipgs,types.NoneType):
            from rotifer.db.ncbi import entrez
            ic = entrez.IPGCursor(progress=False, tries=self.tries)
            ipgs = ic.fetchall(targets)
            targets = targets - ic.missing_ids()
            self.update_missing(data=ic.remove_missing())
        ipgs = ipgs[ipgs.id.isin(ipgs[ipgs.pid.isin(targets) | ipgs.representative.isin(targets)].id)]
        best = rdnu.best_ipgs(ipgs)
        best = best[best[self._target_column].notna()]
        ipgs = ipgs[ipgs[self._target_column].isin(best[self._target_column])]
        missing = targets - self.ipg_proteins(ipgs)
        if missing:
            self.update_missing(missing,"No IPGs",False)
            targets = targets - missing
            if len(targets) == 0:
                return objlist

        # Identify DNA data
        assemblies, nucleotides = rdnu.ipgs_to_dicts(ipgs)

        # Download and parse
        objlist = []
        for accession in assemblies.keys():
            expected = set([ y for x in assemblies[accession].items() for y in x ])
            expected = targets.intersection(expected)

            obj = None
            for attempt in range(0,self.tries):
                # Download and open data file
                error = None
                stream = None
                try:
                    stream = self.fetcher(accession)
                except RuntimeError:
                    error = f'{accession}: {sys.exc_info()[1]}'
                    if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                        logger.exception(error)
                    continue
                except ValueError:
                    error = f'{accession}: {sys.exc_info()[1]}'
                    if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                        logger.exception(error)
                    break
                except:
                    error = f'{accession}: {sys.exc_info()[1]}'
                    if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                        logger.exception(error)
                    continue

                if error or isinstance(stream, types.NoneType):
                    if self.update_missing(expected, error):
                        continue
                    else:
                        break

                # Use parser to process results
                try:
                    obj = self.parser(stream, accession, assemblies[accession])
                    break
                except:
                    error = f"Failed to parse genome {accession}:"
                    if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                        logger.exception(error)

            if isinstance(obj, types.NoneType):
                self.update_missing(expected, error)
            elif len(obj) == 0:
                error = f'No anchors in genome {accession}'
                self.update_missing(expected, error)
            else:
                objlist.append(obj)

        # No data?
        if len(objlist) == 0:
            return seqrecords_to_dataframe([])

        # Concatenate and evaluate
        objlist = pd.concat(objlist, ignore_index=True)

        # Return data
        if len(objlist) > 0:
            self.remove_missing(self.getids(objlist, ipgs=ipgs))
        return objlist

    def fetcher(self, accession):
        if not self.eukaryotes:
            import rotifer.db.ncbi as ncbi
            contigs, report = self.genome_report(accession)
            if len(report) > 0:
                tc = ncbi.TaxonomyCursor(sleep_between_tries=15, tries=5)
                taxonomy = tc[report.loc['taxid'][0]] # Fetching from database
                if taxonomy.loc[0,"superkingdom"] == "Eukaryota":
                    raise ValueError(f"Eukaryotic genome {accession} ignored.")
        stream = self.open_genome(accession)
        if isinstance(stream,types.NoneType):
            raise ValueError(f'Unable to access files for genome {accession}.')
        return stream

    def parser(self, stream, accession, proteins):
        data = super().parser(stream, accession)
        data = data.neighbors(
            data[self.column].isin(proteins.keys()),
            before = self.before,
            after = self.after,
            min_block_distance = self.min_block_distance,
            strand = self.strand,
            fttype = self.fttype,
        )
        data.assembly = accession
        data['replaced'] = data.pid.replace(proteins)
        return data

    def worker(self, chunk):
        result = []
        for args in chunk:
            df = self.__getitem__(*args)
            if len(df) == 0:
                continue
            for x in df.groupby('block_id'):
                result.append(x[1])
        # Make sure some content, even if empty, is always returned
        if len(result) == 0:
            result = [ seqrecords_to_dataframe([]) ]
        return {"result":result,"missing":self.remove_missing()}

    def splitter(self, accessions, ipgs):
        size = self.batch_size
        if size == None or size == 0:
            size = max(int(ipgs[self._target_column].nunique()/self.threads),1)
        batch = []
        for x, y in ipgs.groupby(self._target_column):
            proteins = accessions.intersection(self.ipg_proteins(y))
            batch.append((proteins, y.copy()))
        batch = [ batch[x:x+size] for x in range(0,len(batch),size) ]
        return batch

    def fetchone(self, accessions, ipgs=None):
        """
        Asynchronously fetch gene neighborhoods from NCBI.

        Parameters
        ----------
        accessions: list of strings
          NCBI protein identifiers
        ipgs : Pandas dataframe
          This parameter may be used to avoid downloading IPGs
          from NCBI. Example:

          >>> from rotifer.db.ncbi import entrez
          >>> from rotifer.db.ncbi import ftp
          >>> ic = ncbi.IPGCursor(batch_size=1)
          >>> gnc = ftp.GeneNeighborhoodCursor(progress=True)
          >>> i = ic.fetchall(['WP_063732599.1'])
          >>> n = gnc.fetchall(['WP_063732599.1'], ipgs=i)

        Returns
        -------
        A generator for rotifer.genome.data.NeighborhoodDF objects
        """
        from concurrent.futures import ProcessPoolExecutor, as_completed

        # Make sure no identifiers are used twice
        targets = self.parse_ids(accessions)

        # Make sure we have usable IPGs
        if isinstance(ipgs,types.NoneType):
            from rotifer.db.ncbi import entrez
            if self.progress:
                logger.warn(f'Downloading IPGs for {len(targets)} proteins...')
            ic = entrez.IPGCursor(progress=self.progress, tries=self.tries)
            ipgs = ic.fetchall(targets)
            targets = targets - ic.missing_ids()
            self.update_missing(data=ic.remove_missing())
        valid = ipgs.pid.isin(targets) | ipgs.representative.isin(targets)
        if not valid.all():
            valid = ipgs[valid].id.drop_duplicates()
            ipgs = ipgs[ipgs.id.isin(valid)]
        if self.progress:
            logger.warn(f'Processing {len(ipgs)} rows of {ipgs.id.nunique()} IPGs.')

        # Check for proteins without IPGs
        missing = targets - self.ipg_proteins(ipgs)
        if missing:
            self.update_missing(missing, error="Not found in IPGs", retry=False)
            targets = targets - missing
        if len(ipgs) == 0:
            return [seqrecords_to_dataframe([])]

        # Select best IPGs
        best = rdnu.best_ipgs(ipgs)
        missing = best[best[self._target_column].isna()]
        best = best[best[self._target_column].notna()]
        if len(missing):
            err = missing.melt(id_vars='nucleotide', value_vars=['pid','representative'], value_name='pids', var_name='class')
            err = err[err.pids.isin(targets)]
            err.rename({'nucleotide':'error'}, axis=1, inplace=1)
            err['class'] = self.__name__
            err['retry'] = err.error.notna()
            err['error'] = np.where(
                    err.error.isna(),
                    "No nucleotide or assembly for protein " + err.pids,
                    "Fetch protein " + err.pids + " from nucleotide " + err.error.fillna("")
            )
            err.retry = err.drop(['pids'], axis=1).values.tolist()
            err = err.set_index('pids')[['retry']].retry.to_dict()
            self.update_missing(data=err)
            targets = targets - self.missing_ids(retry=False)

        # filter good IPGs for the best assemblies
        assemblies = ipgs[ipgs[self._target_column].isin(best[self._target_column])]

        # Indexing proteins and IPGs
        pid2ipg = ipgs.melt(id_vars='id', value_vars=['pid','representative'], value_name='pids')
        pid2ipg.drop_duplicates(['id','pids'], inplace=True)
        ipg2targets = pid2ipg[pid2ipg.pids.isin(targets)].groupby('id').agg({'pids':'unique'}).pids.to_dict()
        pid2ipg = pid2ipg.set_index('pids').id.to_dict()

        # Split jobs and execute
        genomes = set(assemblies.assembly.unique())
        with ProcessPoolExecutor(max_workers=self.threads) as executor:
            tasks = []
            missing = self.remove_missing()
            for chunk in self.splitter(targets, assemblies):
                tasks.append(executor.submit(self.worker, chunk))
            self.update_missing(data=missing)

            if self.progress:
                m = f'Downloading {len(genomes)} genomes for {len(targets)} proteins in {len(tasks)} batches, using {self.threads} threads ({self.batch_size} targets/batch)'
                logger.warn(m)
                tqdmobj = tqdm(total=len(genomes), initial=0)

            # Actually processing batches
            try:
                for x in as_completed(tasks):
                    data = x.result()
                    for acc in data['missing'].keys():
                        if x in pid2ipg and pid2ipg[x] in completed:
                            data['missing'].pop(acc, None)
                    self.update_missing(data=data['missing'])
                    for obj in data['result']:
                        found = self.genome_ids(obj)
                        done = genomes.intersection(found)
                        if self.progress and len(done) > 0:
                            tqdmobj.update(len(done))
                        genomes = genomes - done
                        for acc in self.getids(obj):
                            if acc in pid2ipg:
                                if pid2ipg[acc] in ipg2targets:
                                    found = targets.intersection(ipg2targets[pid2ipg[acc]])
                                    self.remove_missing(found)
                                    del(ipg2targets[pid2ipg[acc]])
                        yield obj
            finally:
                if self.progress:
                    tqdmobj.close()

