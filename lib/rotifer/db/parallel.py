# External libraries
import sys
import types
from tqdm import tqdm
from copy import deepcopy

# Rotifer
import rotifer
import rotifer.db.core
logger = rotifer.logging.getLogger(__name__)

class SimpleParallelProcessCursor(rotifer.db.core.BaseCursor):
    """
    Split input into batches and fetch data using parallel processes.

    Parameters
    ----------
    batch_size: int, default 1
      Number of accessions per batch
    threads: integer, default 3
      Number of simultaneous threads to run
    progress: boolean, deafult False
      Whether to print a progress bar
    tries: int, default 3
      Number of attempts to download each batch

    """
    def __init__(self, progress=True, tries=3, batch_size=None, threads=10, *args, **kwargs):
        super().__init__(progress=progress, *args, **kwargs)
        self.tries = tries
        self.batch_size = batch_size
        self.threads = threads

    def __getitem__(self, accession, *args, **kwargs):
        """
        Dictionary-like access to a database entry.

        Parameters
        ----------
        accession: string
          Database entry identifier.

        Returns
        -------
        Objects created by the parser() method
        """
        obj = None
        error = None
        targets = self.parse_ids(accession)
        for attempt in range(0,self.tries):
            try:
                stream = self.fetcher(targets, *args, **kwargs)
            except:
                error = f'{sys.exc_info()[1]}'
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(error)
                giveup = False
                for substr in self.giveup:
                    if substr in error:
                        giveup = True
                        break
                if giveup:
                    break
                else:
                    continue

            if isinstance(stream,types.NoneType):
                error = f"Empty stream for accession(s) {accession}"
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(error)

            try:
                obj = self.parser(stream, targets, *args, **kwargs)
                break
            except:
                error = f'{sys.exc_info()[1]}'
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(f"Parser failed for {accession}")
                giveup = False
                for substr in self.giveup:
                    if substr in error:
                        giveup = True
                        break
                if giveup:
                    break
                else:
                    continue

        # Search for missing accessions
        found = set()
        if isinstance(obj,types.NoneType) or len(obj) == 0:
            self.update_missing(targets, error or "Empty object returned")
        else:
            found = self.getids(obj)
        if found:
            self.remove_missing(found)
        missing = targets - found
        if missing:
            self.update_missing(missing, error or f'Not found.')

        return obj

    def parser(self, stream, accession, *args, **kwargs):
        """
        Create instances of objects by processing the input
        stream generated by fetcher().

        This method is called by __getitem__().

        Parameters
        ----------
        stream: open file-like handle
          Object returned by fetcher().
          Should implement a file-like interface
        accession: string
          Database entry(ies) identifier(s)

        """
        raise NotImplementedError(f'Method parser() must be implemented by descendants')

    def fetcher(self, accession, *args, **kwargs):
        """
        Access remote data as a data stream.

        This method is called by __getitem__().
        """
        raise NotImplementedError(f'Method fetcher() must be implemented by descendants')

    def worker(self, accessions, *args, **kwargs):
        """
        Single-threaded method that will process the data
        batches generated by splitter().

        Parameters
        ----------
        accessions: list of strings
          List of database identifiers

        Returns
        -------
        List of objects of the same class as those 
        generated by self.__getitem__()
        """
        targets = sorted(list(self.parse_ids(accessions)))
        if self.maxgetitem > 1:
            targets = [ targets[x:x+self.maxgetitem] for x in range(0,len(targets),self.maxgetitem) ]
        stack = []
        for chunk in targets:
            obj = self.__getitem__(chunk, *args, **kwargs)
            if isinstance(obj,types.NoneType):
                continue
            if len(obj) == 0:
                continue
            if isinstance(obj,list):
                stack.extend(obj)
            else:
                stack.append(obj)
        return {"result":stack,"missing":self._missing}

    def splitter(self, accessions, *args, **kwargs):
        """
        Convert a list of database identifiers to batches of objects.

        The implementation provided here divides the input into
        lists of length equal to ```batch_size``` and returns
        a generator.

        This method is called by fetchone() to define the datasets
        processed by worker() in each parallel batch.
        """
        from rotifer.devel.alpha.gian_func import chunks
        size = self.batch_size
        if size == None or size == 0:
            size = max(int(len(accessions) / self.threads),1)
        return chunks(list(accessions), size)

    def fetchone(self, accessions, *args, **kwargs):
        """
        Asynchronously fetch data in random order.

        Parameters
        ----------
        accessions: list of strings
          Database accessions

        Returns
        -------
        A generator
        """
        from concurrent.futures import ProcessPoolExecutor, as_completed

        # Split jobs and execute
        targets = self.parse_ids(accessions)
        with ProcessPoolExecutor(max_workers=self.threads) as executor:
            if self.progress:
                p = tqdm(total=len(targets), initial=0)
            tasks = []
            missing = self.remove_missing()
            for chunk in self.splitter(list(targets), *args, **kwargs):
                tasks.append(executor.submit(self.worker, chunk))
            self.update_missing(data=missing)
            completed = set()
            for x in as_completed(tasks):
                data = x.result()
                for acc in completed.intersection(data['missing'].keys()):
                    data['missing'].pop(acc, None)
#                for s in data['missing'].iterrows():
#                    if s[0] in targets and s[0] not in completed:
#                        self.missing.loc[s[0]] = s[1].to_list()
                self.update_missing(data=data['missing'])
                result = data['result']
                if isinstance(result,types.NoneType):
                    continue
                if not isinstance(result,list):
                    result = [result]
                for obj in result:
                    found = self.getids(obj, *args, **kwargs)
                    done = targets.intersection(found) - completed
                    if len(done) > 0:
                        completed.update(done)
                        self.remove_missing(done)
                        if self.progress:
                            p.update(len(done))
                    yield obj

    def fetchall(self, accessions, *args, **kwargs):
        """
        Fetch all data.
        Note: input order is not preserved.

        Parameters
        ----------
        accessions: list of strings
         Database entry identifiers 
        """
        return list(self.fetchone(accessions, *args, **kwargs))

