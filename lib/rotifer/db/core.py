import os
import sys
import types
import typing
import pandas as pd
from tqdm import tqdm
import rotifer
logger = rotifer.logging.getLogger(__name__)

class BaseCursor:
    """
    Generic database cursor abstract interface.

    Parameters
    ----------
    batch_size: int, default 1
      Number of accessions per batch
    threads: integer, default 3
      Number of simultaneous threads to run
    progress: boolean, deafult False
      Whether to print a progress bar
    tries: int, default 3
      Number of attempts to download each batch

    """
    def __init__(
            self,
            progress=False,
            tries=3,
            batch_size=None,
            threads=10
            ):
        self.progress = progress
        self.tries = tries
        self.batch_size = batch_size
        self.threads = threads
        self.missing = set()
        self.__name__ = str(type(self)).split("'")[1]

    def __getitem__(self, accession, *args, **kwargs):
        """
        Fetch data from the database.

        Parameters
        ----------
        accession: string
          Database entry identifier.

        """
        return NotImplementedError(f'Method __getitem__() must be implemented by descendants')

    def fetchone(self, accessions, *args, **kwargs):
        """
        Asynchronously fetch sequences data from a database.
        Note: input order is not preserved.

        Parameters
        ----------
        accessions: list of strings
          Database entry identifiers.

        Returns
        -------
        A generator for Bio.SeqRecord objects
        """
        return NotImplementedError(f'Method fetchone() must be implemented by descendants')

    def fetchall(self, accessions, *args, **kwargs):
        """
        Fetch all data.
        Note: input order is not preserved.

        Parameters
        ----------
        accessions: list of strings
         Database entry identifiers 
        """
        return NotImplementedError(f'Method fetchall() must be implemented by descendants')

class SimpleParallelProcessCursor(BaseCursor):
    """
    Split input into batches and fetch data using parallel processes.

    Parameters
    ----------
    batch_size: int, default 1
      Number of accessions per batch
    threads: integer, default 3
      Number of simultaneous threads to run
    progress: boolean, deafult False
      Whether to print a progress bar
    tries: int, default 3
      Number of attempts to download each batch

    """
    def __init__(
            self,
            batch_size=200,
            threads=5,
            progress=False,
            tries=3,
        ):
        self.batch_size = batch_size
        self.threads = threads
        self.progress = progress
        self.tries = tries
        #self.missing = pd.DataFrame(columns=['error','class'])
        self.missing = set()

    def getids(self, obj):
        """
        Extract accessions from the objects generated by parser().

        Returns
        -------
        A set of strings.
        """
        return NotImplementedError(f'Method getids() must be implemented by descendants')

    def update_missing(self, accessions, error):
        err = [error,str(type(self)).split("'")[1]]
        if not isinstance(accessions,typing.Iterable) or isinstance(accessions,str):
            accessions = [accessions]
        for x in accessions:
            self.missing.loc[x] = err

    def __getitem__(self, accession, *args, **kwargs):
        """
        Dictionary-like access to a database entry.

        Parameters
        ----------
        accession: string
          Database entry identifier.

        """
        obj = None
        for attempt in range(0,self.tries):
            try:
                stream = self.fetcher(accession, *args, **kwargs)
            except RuntimeError:
                error = f'Runtime error for accession {accession}'
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(error)
                continue
            except:
                error = f"Fetcher failed for {accession}"
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(error)
                continue

            if isinstance(stream,types.NoneType):
                error = f"Empty stream for {accession}"
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(error)

            try:
                obj = self.parser(stream, accession, *args, **kwargs)
                break
            except:
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(f"Parser failed for {accession}")
                continue

        # Search for missing accessions
        if isinstance(obj,types.NoneType):
            return obj
        found = self.getids(obj)
        if isinstance(accession, typing.Iterable) and not isinstance(accession,str):
            accession = set(accession)
        else:
            accession = {accession}
        missing = accession - found
        if missing:
            logger.debug(f'''Unable to fetch data for accessions {accession}''')
            self.missing.update(missing)

        return obj

    def parser(self, stream, accession, *args, **kwargs):
        """
        Create instances of objects by processing the input
        stream generated by fetcher().

        This method is called by __getitem__().
        
        Parameters
        ----------
        stream: open file-like handle
          Object returned by fetcher().
          Should implement a file-like interface
        accession: string
          Database entry(ies) identifier(s)

        """
        return NotImplementedError(f'Method parser() must be implemented by descendants')

    def fetcher(self, accession, *args, **kwargs):
        """
        Access remote data as a data stream.
        
        This method is called by __getitem__().
        """
        return NotImplementedError(f'Method fetcher() must be implemented by descendants')

    def worker(self, accessions, *args, **kwargs):
        """
        Single-threaded method that will process the data
        batches generated by splitter().

        Parameters
        ----------
        accessions: list of strings
          List of database identifiers

        Returns
        -------
        List of objects of the same class as generated by self.__getitem__()
        """
        stack = []
        for accession in accessions:
            objlist = self[accession]
            if isinstance(objlist,types.NoneType):
                continue
            if len(objlist) != 0:
                if isinstance(objlist,list):
                    stack.extend(objlist)
                else:
                    stack.append(objlist)
        return stack

    def splitter(self, accessions, *args, **kwargs):
        """
        Convert a list of database identifiers to batches of objects.

        The implementation provided here divides the input into
        lists of length equal to ```batch_size``` and returns
        a generator.

        This method is called by fetchone() to define the datasets
        processed by worker() in each parallel batch.
        """
        from rotifer.devel.alpha.gian_func import chunks
        size = self.batch_size
        if size == None or size == 0:
            size = max(int(len(accessions) / self.threads),1)
        return chunks(list(accessions), size)

    def fetchone(self, accessions, *args, **kwargs):
        """
        Asynchronously fetch genomes in random order.

        Parameters
        ----------
        accessions: list of strings
          NCBI genomes accessions

        Returns
        -------
        A generator for rotifer.genome.data.NeighborhoodDF objects
        """
        from concurrent.futures import ProcessPoolExecutor, as_completed

        # Split jobs and execute
        if not isinstance(accessions,typing.Iterable) or isinstance(accessions,str):
            accessions = [accessions]
        accessions = set(accessions)
        self.missing = self.missing.union(accessions)
        with ProcessPoolExecutor(max_workers=self.threads) as executor:
            if self.progress:
                p = tqdm(total=len(accessions), initial=0)
            tasks = []
            for chunk in self.splitter(list(accessions), *args, **kwargs):
                tasks.append(executor.submit(self.worker, chunk))
            todo = accessions
            for x in as_completed(tasks):
                for obj in x.result():
                    found = self.getids(obj)
                    done = todo.intersection(found)
                    self.missing = self.missing - done
                    if self.progress and len(done) > 0:
                        p.update(len(done))
                    todo = todo - done
                    yield obj

    def fetchall(self, accessions, *args, **kwargs):
        """
        Fetch all data.
        Note: input order is not preserved.

        Parameters
        ----------
        accessions: list of strings
         Database entry identifiers 
        """
        return list(self.fetchone(accessions, *args, **kwargs))

class BaseGeneNeighborhoodCursor(BaseCursor):
    def __init__(
            self,
            column = 'pid',
            before = 7,
            after = 7,
            min_block_distance = 0,
            strand = None,
            fttype = 'same',
            eukaryotes = False,
            exclude_type = ['gene','mRNA','source'],
            autopid = True,
            codontable = 'Bacterial',
            progress=False,
            tries=3,
            batch_size=None,
            threads=10
        ):
        super().__init__(
            progress = progress,
            tries = tries,
            batch_size = batch_size,
            threads = threads,
        )
        self.column = column
        self.before = before
        self.after = after
        self.min_block_distance = min_block_distance
        self.strand = strand
        self.fttype = fttype
        self.eukaryotes = eukaryotes
        self.exclude_type = exclude_type
        self.autopid = autopid
        self.codontable = codontable
        self.missing = pd.DataFrame(columns=["noipgs","eukaryote","assembly","error",'class'])

    def update_missing(self, accessions, assembly, error):
        err = [False,False,assembly,error,str(type(self)).split("'")[1]]
        if "Eukaryotic" in error:
            err[1] = True
        if "IPG" in error:
            err[0] = True
        if not isinstance(accessions,typing.Iterable) or isinstance(accessions,str):
            accessions = [accessions]
        for x in accessions:
            self.missing.loc[x] = err

    def getids(self, obj, ipgs=None):
        # extract ids from dataframe
        if isinstance(obj,pd.DataFrame):
            # Load columns
            columns = self.column
            if not isinstance(columns,typing.Iterable) or isinstance(columns,str):
                columns = [columns]
            else:
                columns = list(columns)

            # when searching for proteins, ensure all columns with protein IDs are used
            pids = ['pid','replaced','representative']
            if set(columns).intersection(pids):
                columns += [ x for x in pids if x not in columns ]

            # Load identifiers from object
            ids = set()
            for col in columns:
                if col in obj.columns:
                    ids.update(set(obj[col].dropna()))

        # If obj is a list
        elif not isinstance(obj,typing.Iterable) or isinstance(obj,str):
            ids = {obj}
        elif isinstance(obj,typing.Iterable):
            ids = set(obj)
        else:
            logger.error(f'Unknown type {type(obj)}')

        # Add synonyms from IPGs
        if not isinstance(ipgs,types.NoneType):
            ipgids = ipgs[ipgs.pid.isin(ids) | ipgs.representative.isin(ids)].id
            ipgids = ipgs[ipgs.id.isin(ipgids)]
            ids.update(ipgids.pid.dropna())
            ids.update(ipgids.representative.dropna())

        return ids

if __name__ == '__main__':
    pass
