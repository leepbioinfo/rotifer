import os
import sys
import types
import typing
import importlib
import pandas as pd
from tqdm import tqdm
from copy import deepcopy
import rotifer
logger = rotifer.logging.getLogger(__name__)

class BaseCursor:
    """
    Generic database cursor abstract interface.

    Parameters
    ----------
    batch_size: int, default 1
      Number of accessions per batch
    threads: integer, default 3
      Number of simultaneous threads to run
    progress: boolean, deafult False
      Whether to print a progress bar
    tries: int, default 3
      Number of attempts to download each batch

    """
    def __init__(
            self,
            progress=False,
            tries=3,
            batch_size=None,
            threads=10,
            *args, **kwargs
            ):
        self.progress = progress
        self.tries = tries
        self.batch_size = batch_size
        self.threads = threads
        self.missing = set()
        self.__name__ = str(type(self)).split("'")[1]

    def getids(self, obj):
        """
        Extract accessions from the objects generated by parser().

        Returns
        -------
        A set of strings.
        """
        return NotImplementedError(f'Method getids() must be implemented by descendants')

    def parse_ids(self, accessions, as_string=True):
        """
        Convert a list of accessions into a set object

        Usage
        -----
        setobj = cursor.parse_ids(["acc1","acc2"])

        Parameters
        ----------
        as_string : boolean, default True
          Convert individual accessions to strings

        Rational
        --------
        Cursors usually receive a list of valid database
        identifiers as input but such a list can be given
        as strings, lists, Pandas series or other objects.

        This method ensures that the input converges into
        a standard representation, i.e. a Python set.
        """
        targets = deepcopy(accessions)
        if isinstance(targets,str):
            targets = targets.split(",") # Useful for Entrez... remove?
        elif not isinstance(targets,typing.Iterable):
            targets = [ targets ]
        if as_string:
            targets = [ str(x) for x in targets ]
        targets = set(targets)
        return targets

    def __getitem__(self, accession, *args, **kwargs):
        """
        Fetch data from the database.

        Parameters
        ----------
        accession: string
          Database entry identifier.

        """
        raise NotImplementedError(f'Method __getitem__() must be implemented by descendants')

    def fetchone(self, accessions, *args, **kwargs):
        """
        Asynchronously fetch sequences data from a database.
        Note: input order is not preserved.

        Parameters
        ----------
        accessions: list of strings
          Database entry identifiers.

        Returns
        -------
        A generator for Bio.SeqRecord objects
        """
        raise NotImplementedError(f'Method fetchone() must be implemented by descendants')

    def fetchall(self, accessions, *args, **kwargs):
        """
        Fetch all data.
        Note: input order is not preserved.

        Parameters
        ----------
        accessions: list of strings
         Database entry identifiers 
        """
        raise NotImplementedError(f'Method fetchall() must be implemented by descendants')

class SimpleParallelProcessCursor(BaseCursor):
    """
    Split input into batches and fetch data using parallel processes.

    Parameters
    ----------
    batch_size: int, default 1
      Number of accessions per batch
    threads: integer, default 3
      Number of simultaneous threads to run
    progress: boolean, deafult False
      Whether to print a progress bar
    tries: int, default 3
      Number of attempts to download each batch

    """
    def __init__(
            self,
            batch_size=200,
            threads=5,
            progress=False,
            tries=3,
        ):
        self.batch_size = batch_size
        self.threads = threads
        self.progress = progress
        self.tries = tries
        #self.missing = pd.DataFrame(columns=['error','class'])
        self.missing = set()

    def update_missing(self, accessions, error):
        err = [error,str(type(self)).split("'")[1]]
        if not isinstance(accessions,typing.Iterable) or isinstance(accessions,str):
            accessions = [accessions]
        self.missing.update(accessions)

    def __getitem__(self, accession, *args, **kwargs):
        """
        Dictionary-like access to a database entry.

        Parameters
        ----------
        accession: string
          Database entry identifier.

        """
        obj = None
        for attempt in range(0,self.tries):
            try:
                stream = self.fetcher(accession, *args, **kwargs)
            except RuntimeError:
                error = f'Runtime error for accession {accession}'
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(error)
                continue
            except:
                error = f"Fetcher failed for {accession}"
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(error)
                continue

            if isinstance(stream,types.NoneType):
                error = f"Empty stream for {accession}"
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(error)

            try:
                obj = self.parser(stream, accession, *args, **kwargs)
                break
            except:
                if logger.getEffectiveLevel() <= rotifer.logging.DEBUG:
                    logger.exception(f"Parser failed for {accession}")
                continue

        # Search for missing accessions
        if isinstance(obj,types.NoneType):
            return obj
        found = self.getids(obj)
        if isinstance(accession, typing.Iterable) and not isinstance(accession,str):
            accession = set(accession)
        else:
            accession = {accession}
        missing = accession - found
        if missing:
            logger.debug(f'''Unable to fetch data for accessions {accession}''')
            self.missing.update(missing)

        return obj

    def parser(self, stream, accession, *args, **kwargs):
        """
        Create instances of objects by processing the input
        stream generated by fetcher().

        This method is called by __getitem__().
        
        Parameters
        ----------
        stream: open file-like handle
          Object returned by fetcher().
          Should implement a file-like interface
        accession: string
          Database entry(ies) identifier(s)

        """
        raise NotImplementedError(f'Method parser() must be implemented by descendants')

    def fetcher(self, accession, *args, **kwargs):
        """
        Access remote data as a data stream.
        
        This method is called by __getitem__().
        """
        raise NotImplementedError(f'Method fetcher() must be implemented by descendants')

    def worker(self, accessions, *args, **kwargs):
        """
        Single-threaded method that will process the data
        batches generated by splitter().

        Parameters
        ----------
        accessions: list of strings
          List of database identifiers

        Returns
        -------
        List of objects of the same class as generated by self.__getitem__()
        """
        stack = []
        for accession in accessions:
            objlist = self[accession]
            if isinstance(objlist,types.NoneType):
                continue
            if len(objlist) != 0:
                if isinstance(objlist,list):
                    stack.extend(objlist)
                else:
                    stack.append(objlist)
        return stack

    def splitter(self, accessions, *args, **kwargs):
        """
        Convert a list of database identifiers to batches of objects.

        The implementation provided here divides the input into
        lists of length equal to ```batch_size``` and returns
        a generator.

        This method is called by fetchone() to define the datasets
        processed by worker() in each parallel batch.
        """
        from rotifer.devel.alpha.gian_func import chunks
        size = self.batch_size
        if size == None or size == 0:
            size = max(int(len(accessions) / self.threads),1)
        return chunks(list(accessions), size)

    def fetchone(self, accessions, *args, **kwargs):
        """
        Asynchronously fetch genomes in random order.

        Parameters
        ----------
        accessions: list of strings
          NCBI genomes accessions

        Returns
        -------
        A generator for rotifer.genome.data.NeighborhoodDF objects
        """
        from concurrent.futures import ProcessPoolExecutor, as_completed

        # Split jobs and execute
        targets = self.parse_ids(accessions)
        self.missing = self.missing.union(targets)
        with ProcessPoolExecutor(max_workers=self.threads) as executor:
            if self.progress:
                p = tqdm(total=len(targets), initial=0)
            tasks = []
            for chunk in self.splitter(list(targets), *args, **kwargs):
                tasks.append(executor.submit(self.worker, chunk))
            todo = deepcopy(targets)
            for x in as_completed(tasks):
                for obj in x.result():
                    found = self.getids(obj)
                    done = todo.intersection(found)
                    self.missing = self.missing - done
                    if self.progress and len(done) > 0:
                        p.update(len(done))
                    todo = todo - done
                    yield obj

    def fetchall(self, accessions, *args, **kwargs):
        """
        Fetch all data.
        Note: input order is not preserved.

        Parameters
        ----------
        accessions: list of strings
         Database entry identifiers 
        """
        return list(self.fetchone(accessions, *args, **kwargs))

class BaseDelegatorCursor(BaseCursor):
    def __init__(self, methods=[], progress=False, tries=3, batch_size=None, threads=10, *args, **kwargs):
        super().__init__(progress=progress, tries=tries, batch_size=batch_size, threads=threads, *args, **kwargs)
        self.methods = methods
        self.load_cursor_modules()
        self.reset_cursors()

    def load_cursor_modules(self):
        import inspect
        mymodule = inspect.getmodule(self)
        self._cursor_modules = dict()

        # Check configuration
        try:
            myconfig = getattr(mymodule,'config')
        except:
            error = f'Module {mymodule.__name__} has no configuration! Blame the developer!'
            logger.error(error)
            raise ValueError(f'No attribute "config" in module {mymodule.__name__}')
        if 'cursor_methods' not in myconfig:
            error = f'Missing dictionary of methods in configuration of module {mymodule.__name__}'
            logger.error(error)
            raise ValueError(error)

        # Load modules
        for module in self.methods:
            if module not in myconfig['cursor_methods']:
                error = f'Method {module} not found in {mymodule.__name__}.config["cursor_methods"]'
                logger.error(error)
                raise ValueError(error)
            module_name = myconfig['cursor_methods'][module]
            try:
                self._cursor_modules[module] = importlib.import_module(module_name)
            except:
                logger.error(f'Unable to load module {module_name}: %s.', exc_info=1)
                raise ImportError(f'Unable to load module {module_name}')

    def reset_cursors(self):
        myname = str(type(self)).split("'")[1].split(".")[-1]
        if hasattr(self,"_shared_attributes"):
            kwargs = { x: getattr(self,x) for x in self._shared_attributes }
        else:
            kwargs = dict()
        self.cursors = dict()
        for modulename in self._cursor_modules:
            module = self._cursor_modules[modulename]
            try:
                cursorClass = getattr(module,myname)
            except:
                logger.error(f'Module {module.__name__} does not define a {myname} class')
                continue
            self.cursors[modulename] = cursorClass(**kwargs)

    def __setattr__(self, name, value):
        super().__setattr__(name, value)
        if hasattr(self,'cursors') and hasattr(self,'_shared_attributes') and name in self._shared_attributes:
            for cursor in self.cursors.values():
                if hasattr(cursor,name):
                    cursor.__setattr__(name,value)

class BaseSerialDelegatorCursor(BaseDelegatorCursor):
    def __init__(self, methods=[], progress=False, tries=3, batch_size=None, threads=10, *args, **kwargs):
        super().__init__(methods=methods, progress=progress, tries=tries, batch_size=batch_size, threads=threads, *args, **kwargs)

    def __getitem__(self, accessions, *args, **kwargs):
        """
        Dictionary-like access to data.

        Usage
        -----
        General template of the __getitem__ method,
        for any BaseCursor child class:

        >>> import rotifer.db.ncbi as ncbi
        >>> tc = ncbi.TaxonomyCursor(progress=True)
        >>> t = tc[2599]

        Parameters
        ----------
        accessions: list of strings
          Database identifiers.

        Returns
        -------
        Pandas dataframe
        """
        targets = self.parse_ids(accessions)
        todo = deepcopy(targets)

        # Call cursors
        data = []
        for i in range(0,len(self.methods)):
            if len(todo) == 0:
                break
            cursorName = self.methods[i]
            if cursorName in self.cursors:
                cursor = self.cursors[cursorName]
            else:
                continue
            result = cursor.__getitem__(todo, *args, **kwargs)
            found = self.getids(result, *args, **kwargs)
            for j in range(0,i+1):
                c = self.methods[j]
                if c not in self.cursors:
                    continue
                self.cursors[c].missing -= found
            todo = deepcopy(cursor.missing)
            if isinstance(result, list):
                data.extend(result)
            else:
                data.append(result)
        if len(targets) == 1 and len(data) == 1:
            data = data[0]
        return data

    def fetchone(self, accessions, *args, **kwargs):
        """
        Get a generator to fetch data for iteratively.

        Parameters
        ----------
        accessions: list of strings
          Database identifiers.

        Returns
        -------
        Generator of Pandas dataframes
        """
        targets = self.parse_ids(accessions)
        todo = deepcopy(targets)

        # Call cursors
        for i in range(0,len(self.methods)):
            if len(todo) == 0:
                break
            cursorName = self.methods[i]
            if cursorName in self.cursors:
                cursor = self.cursors[cursorName]
            else:
                continue
            for result in cursor.fetchone(todo, *args, **kwargs):
                found = self.getids(result, *args, **kwargs)
                for j in range(0,i+1):
                    c = self.methods[j]
                    if c not in self.cursors:
                        continue
                    self.cursors[c].missing -= found
                todo = deepcopy(cursor.missing)
                yield result

    def fetchall(self, accessions, *args, **kwargs):
        """
        Fetch data for all accessions.

        Parameters
        ----------
        accessions: list of database identifiers
          Database identifiers.

        Returns
        -------
        Pandas dataframe
        """
        targets = self.parse_ids(accessions)
        stack = []
        for data in self.fetchone(targets, *args, **kwargs):
            stack.append(data)
        return stack

class BaseGeneNeighborhoodCursor(BaseCursor):
    def __init__(
            self,
            column = 'pid',
            before = 7,
            after = 7,
            min_block_distance = 0,
            strand = None,
            fttype = 'same',
            eukaryotes = False,
            exclude_type = ['gene','mRNA','source'],
            autopid = True,
            codontable = 'Bacterial',
            progress=False,
            tries=3,
            batch_size=None,
            threads=10
        ):
        super().__init__(
            progress = progress,
            tries = tries,
            batch_size = batch_size,
            threads = threads,
        )
        self.column = column
        self.before = before
        self.after = after
        self.min_block_distance = min_block_distance
        self.strand = strand
        self.fttype = fttype
        self.eukaryotes = eukaryotes
        self.exclude_type = exclude_type
        self.autopid = autopid
        self.codontable = codontable
        self.missing = pd.DataFrame(columns=["noipgs","eukaryote","assembly","error",'class'])

    def update_missing(self, accessions, assembly, error):
        err = [False,False,assembly,error,str(type(self)).split("'")[1]]
        if "Eukaryotic" in error:
            err[1] = True
        if "IPG" in error:
            err[0] = True
        if not isinstance(accessions,typing.Iterable) or isinstance(accessions,str):
            accessions = [accessions]
        for x in accessions:
            self.missing.loc[x] = err

    def getids(self, obj, ipgs=None):
        # extract ids from dataframe
        if isinstance(obj,pd.DataFrame):
            # Load columns
            columns = self.column
            if not isinstance(columns,typing.Iterable) or isinstance(columns,str):
                columns = [columns]
            else:
                columns = list(columns)

            # when searching for proteins, ensure all columns with protein IDs are used
            pids = ['pid','replaced','representative']
            if set(columns).intersection(pids):
                columns += [ x for x in pids if x not in columns ]

            # Load identifiers from object
            ids = set()
            for col in columns:
                if col in obj.columns:
                    ids.update(set(obj[col].dropna()))

        # If obj is a list
        elif not isinstance(obj,typing.Iterable) or isinstance(obj,str):
            ids = {obj}
        elif isinstance(obj,typing.Iterable):
            ids = set(obj)
        else:
            logger.error(f'Unknown type {type(obj)}')

        # Add synonyms from IPGs
        if not isinstance(ipgs,types.NoneType):
            ipgids = ipgs[ipgs.pid.isin(ids) | ipgs.representative.isin(ids)].id
            ipgids = ipgs[ipgs.id.isin(ipgids)]
            ids.update(ipgids.pid.dropna())
            ids.update(ipgids.representative.dropna())

        return ids

if __name__ == '__main__':
    pass
