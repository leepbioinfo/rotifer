import os
import sys
import types
import typing
from tqdm import tqdm
import rotifer
logger = rotifer.logging.getLogger(__name__)

class BaseCursor:
    """
    Generic database cursor abstract interface.

    Parameters
    ----------
    batch_size: int, default 1
      Number of accessions per batch
    threads: integer, default 3
      Number of simultaneous threads to run
    progress: boolean, deafult False
      Whether to print a progress bar
    tries: int, default 3
      Number of attempts to download each batch

    """
    def __init__(
            self,
            progress=False,
            tries=3,
            batch_size=None,
            threads=10
            ):
        self.progress = progress
        self.tries = tries
        self.batch_size = batch_size
        self.threads = threads
        self.missing = set()

    def __getitem__(self, accession, *args, **kwargs):
        """
        Fetch data from the database.

        Parameters
        ----------
        accession: string
          Database entry identifier.

        """
        return NotImplementedError

    def fetch_each(self, accessions, *args, **kwargs):
        """
        Asynchronously fetch sequences data from a database.
        Note: input order is not preserved.

        Parameters
        ----------
        accessions: list of strings
          Database entry identifiers.

        Returns
        -------
        A generator for Bio.SeqRecord objects
        """
        return NotImplementedError

    def fetch_all(self, accessions, *args, **kwargs):
        """
        Fetch all data.
        Note: input order is not preserved.

        Parameters
        ----------
        accessions: list of strings
         Database entry identifiers 
        """
        return NotImplementedError

class SimpleParallelProcessCursor(BaseCursor):
    """
    Split input into batches and fetch data using parallel processes.

    Parameters
    ----------
    batch_size: int, default 1
      Number of accessions per batch
    threads: integer, default 3
      Number of simultaneous threads to run
    progress: boolean, deafult False
      Whether to print a progress bar
    tries: int, default 3
      Number of attempts to download each batch

    """
    def getids(self, obj):
        """
        Extract inut accessions from objects generated by parser().

        Returns
        -------
        A set of strings.
        """
        raise NotImplementedError

    def __getitem__(self, accession, *args, **kwargs):
        """
        Dictionary-like access to a database entry.

        Parameters
        ----------
        accession: string
          Database entry identifier.

        """
        obj = None
        for attempt in range(0,self.tries):
            try:
                stream = self.fetcher(accession, *args, **kwargs)
            except RuntimeError:
                logger.debug(f'Runtime error: '+str(sys.exc_info()[1]))
                continue
            except:
                logger.debug(f"Fetcher failed for {accession}: {sys.exc_info()}")
                continue
            try:
                obj = self.parser(stream, accession, *args, **kwargs)
                break
            except:
                logger.debug(f"Parser failed for {accession}: {sys.exc_info()}")
                continue

        # Search for missing accessions
        found = self.getids(obj)
        if isinstance(accession, typing.Iterable) and not isinstance(accession,str):
            accession = set(accession)
        else:
            accession = {accession}
        missing = accession - found
        if missing:
            logger.debug(f'''Unable to fetch data for accessions {accession}''')
            self.missing.update(missing)

        return obj

    def parser(self, stream, accession, *args, **kwargs):
        """
        Create instances of objects by processing the input
        stream generated by fetcher().

        This method is called by __getitem__().
        
        Parameters
        ----------
        stream: open file-like handle
          Object returned by fetcher().
          Should implement a file-like interface
        accession: string
          Database entry(ies) identifier(s)

        """
        return NotImplementedError

    def fetcher(self, accession, *args, **kwargs):
        """
        Access remote data as a data stream.
        
        This method is called by __getitem__().
        """
        return NotImplementedError

    def worker(self, accessions, *args, **kwargs):
        """
        Single-threaded method that will process the data
        batches generated by splitter().

        Parameters
        ----------
        accessions: list of strings
          List of database identifiers

        Returns
        -------
        List of objects of the same class as generated by self.__getitem__()
        """
        return NotImplementedError

    def splitter(self, accessions, *args, **kwargs):
        """
        Convert a list of database identifiers to batches of objects.

        The implementation provided here divides the input into
        lists of length equal to ```batch_size``` and returns
        a generator.

        This method is called by fetch_each() to define the datasets
        processed by worker() in each parallel batch.
        """
        from rotifer.devel.alpha.gian_func import chunks
        size = self.batch_size
        if size == None or size == 0:
            size = max(int(len(accessions) / self.threads),1)
        return chunks(list(accessions), size)

    def fetch_each(self, accessions, *args, **kwargs):
        """
        Asynchronously fetch genomes in random order.

        Parameters
        ----------
        accessions: list of strings
          NCBI genomes accessions

        Returns
        -------
        A generator for rotifer.genome.data.NeighborhoodDF objects
        """
        from concurrent.futures import ProcessPoolExecutor, as_completed

        # Split jobs and execute
        if not isinstance(accessions,typing.Iterable) or isinstance(accessions,str):
            accessions = [accessions]
        accessions = set(accessions)
        self.missing = self.missing.union(accessions)
        with ProcessPoolExecutor(max_workers=self.threads) as executor:
            if self.progress:
                p = tqdm(total=len(accessions), initial=0)
            tasks = []
            for chunk in self.splitter(list(accessions), *args, **kwargs):
                tasks.append(executor.submit(self.worker, chunk))
            todo = accessions
            for x in as_completed(tasks):
                for obj in x.result():
                    found = self.getids(obj)
                    done = todo.intersection(found)
                    self.missing = self.missing - done
                    if self.progress and len(done) > 0:
                        p.update(len(done))
                    todo = todo - done
                    yield obj

    def fetch_all(self, accessions, *args, **kwargs):
        """
        Fetch all data.
        Note: input order is not preserved.

        Parameters
        ----------
        accessions: list of strings
         Database entry identifiers 
        """
        return list(self.fetch_each(accessions, *args, **kwargs))

if __name__ == '__main__':
    pass
