#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Import required modules
import os
import sys
import argparse
import requests
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from tqdm import tqdm
from urllib.parse import urljoin
from pathlib import Path
import time
from humanize import naturalsize
import shutil

# Base URL for downloading RefSeq genomes
BASE_URL = "https://ftp.ncbi.nlm.nih.gov/genomes/refseq"

# Mapping group names to their respective summary file
GROUPS = {
    "bacteria": "assembly_summary_bacteria.txt",
    "archaea": "assembly_summary_archaea.txt",
    "viral": "assembly_summary_viral.txt"
}

# Function to download the assembly_summary.txt for a group
def download_summary_file(group, force=False):
    url = f"{BASE_URL}/{group}/assembly_summary.txt"
    filename = f"assembly_summary_{group}.txt"
    if not os.path.exists(filename) or force:
        print(f"[INFO] Downloading: {url}")
        r = requests.get(url)
        if r.status_code == 200:
            with open(filename, "w") as f:
                f.write(r.text)
        else:
            raise Exception(f"Failed to download {url}: HTTP {r.status_code}")
    return filename

# Parse the assembly summary file and extract (ftp_path, prefix_path) tuples
def parse_assembly_summary(file_path):
    genomes = []
    with open(file_path, "r") as f:
        for line in f:
            if line.startswith("#"):
                continue  # Skip comments
            parts = line.strip().split("\t")
            if len(parts) >= 20:
                ftp_path = parts[19]
                if "genomes/all/" in ftp_path:
                    sub = ftp_path.split("genomes/all/")[1]
                    prefix_path = "/".join(sub.split("/")[:3])
                    genomes.append((ftp_path, prefix_path))
    return genomes

# Check if a directory is missing or empty
def is_empty_or_missing(path):
    return not os.path.exists(path) or not any(Path(path).rglob("*"))

# Calculate the total size of files inside a directory
def get_total_size(path):
    total = 0
    for dirpath, _, filenames in os.walk(path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            if os.path.isfile(fp):
                try:
                    total += os.path.getsize(fp)
                except FileNotFoundError:
                    continue  # Skip if file disappears during scan
    return total

# Function to download a prefix using rsync
async def run_rsync_download(ftp_path, prefix_path, db_root, semaphore, pbar, download_stats):
    async with semaphore:
        target_dir = os.path.join(db_root, "genomes", "all", prefix_path)
        os.makedirs(target_dir, exist_ok=True)

        rsync_path = f"rsync://ftp.ncbi.nlm.nih.gov/genomes/all/{prefix_path}/"
        cmd = [
            "rsync", "-a", "--ignore-existing", "--timeout=10",
            rsync_path, target_dir
        ]

        # Launch rsync and suppress output
        start = time.time()
        proc = await asyncio.create_subprocess_exec(
          *cmd,
          stdout=asyncio.subprocess.DEVNULL,
          stderr=asyncio.subprocess.DEVNULL
        )
        await proc.communicate()
        elapsed = time.time() - start

        # Update downloaded size and speed statistics
        downloaded_bytes = get_total_size(target_dir)
        download_stats['bytes'] += downloaded_bytes
        download_stats['elapsed'] += elapsed

        current_speed = downloaded_bytes / elapsed if elapsed > 0 else 0

        # Update the progress bar
        pbar.update(1)
        pbar.set_postfix({
            "Downloaded": naturalsize(download_stats['bytes']),
            "Speed": f"{current_speed / (1024**2):.2f} MB/s"
        })

# Main logic to select groups, parse summaries and manage downloads
async def main(args):
    selected = []
    if args.bacteria: selected.append("bacteria")
    if args.archaea: selected.append("archaea")
    if args.viral: selected.append("viral")

    if not selected:
        print("[ERROR] Select at least one group: -b, -a, -v")
        sys.exit(1)

    all_genomes = []
    for group in selected:
        f = download_summary_file(group, args.update_assembly)
        genomes = parse_assembly_summary(f)
        print(f"[INFO] {len(genomes)} genomes found in {group}")
        all_genomes.extend(genomes)

    print(f"[INFO] Total {len(all_genomes)} genomes to check.")

    # Filter genomes to download (only if folder is empty/missing)
    genomes_to_download = []
    for ftp_path, prefix_path in all_genomes:
        target_dir = os.path.join(args.database, "genomes", "all", prefix_path)
        if is_empty_or_missing(target_dir):
            genomes_to_download.append((ftp_path, prefix_path))

    print(f"[INFO] {len(genomes_to_download)} genomes to download.")

    semaphore = asyncio.Semaphore(args.threads)
    tasks = []
    download_stats = {'bytes': 0, 'elapsed': 0.00001}

    # Create a progress bar
    pbar = tqdm(total=len(genomes_to_download), desc="Downloading genomes", unit="genome")
    for ftp_path, prefix_path in genomes_to_download:
        task = asyncio.create_task(
            run_rsync_download(ftp_path, prefix_path, args.database, semaphore, pbar, download_stats)
        )
        tasks.append(task)

    await asyncio.gather(*tasks)
    pbar.close()

# Entry point for the script
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Download RefSeq genomes using rsync if folder is empty")
    parser.add_argument("-d", "--database", default="./db", help="Target download directory")
    parser.add_argument("-b", "--bacteria", action="store_true", help="Include bacterial genomes")
    parser.add_argument("-a", "--archaea", action="store_true", help="Include archaeal genomes")
    parser.add_argument("-v", "--viral", action="store_true", help="Include viral genomes")
    parser.add_argument("-t", "--threads", type=int, default=5, help="Number of concurrent genome downloads")
    parser.add_argument("--update-assembly", action="store_true", help="Force redownload of summary files")
    args = parser.parse_args()

    asyncio.run(main(args))
